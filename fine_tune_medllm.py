# -*- coding: utf-8 -*-
"""Fine Tune MedLLM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/135Fu2o9THOlXLUCvxh1HDiQsrh-IxOLl
"""

print("Fine Tune a Medical Q/A")

import torch
print(f"CUDA available: {torch.cuda.is_available()}")
print(f"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}")
print(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")

import torch
print(f"CUDA available: {torch.cuda.is_available()}")
print(f"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}")
print(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")

!pip install -q unsloth trl peft accelerate bitsandbytes datasets

from datasets import load_dataset
import json

dataset = load_dataset("medmcqa", split="train[:1000]")  # First 1000 samples
print(f"Dataset size: {len(dataset)}")
print("Sample:", dataset[0])

#Format data according to your task - Medical QA format

def format_medical_qa(example):
    question = example['question']
    options = [example['opa'], example['opb'], example['opc'], example['opd']]
    correct_answer = options[example['cop']]

    # Format as instruction-response pairs
    formatted_text = f"### Question: {question}\n### Options: A) {options[0]} B) {options[1]} C) {options[2]} D) {options[3]}\n### Answer: {correct_answer}<|endoftext|>"
    return formatted_text

# Apply formatting to all samples
formatted_data = [format_medical_qa(item) for item in dataset]
print("Formatted sample:")
print(formatted_data[0])

!pip install unsloth trl peft accelerate bitsandbytes

# Load the model with 4-bit quantization

from unsloth import FastLanguageModel

model_name = "unsloth/Phi-3-mini-4k-instruct-bnb-4bit"
max_seq_length = 2048
dtype = None

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name=model_name,
    max_seq_length=max_seq_length,
    dtype=dtype,
    load_in_4bit=True,
)

from datasets import Dataset

train_dataset = Dataset.from_dict({"text": formatted_data})
print(f"Training dataset size: {len(train_dataset)}")

model = FastLanguageModel.get_peft_model(
    model,
    r=64,  # LoRA rank
    target_modules=[
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj",
    ],
    lora_alpha=128,
    lora_dropout=0,
    bias="none",
    use_gradient_checkpointing="unsloth",
    random_state=3407,
    use_rslora=False,
    loftq_config=None,
)

print("LoRA adapters added!")

# ==================== CHUNK 8: Training Configuration ====================
# Set up training parameters

from trl import SFTTrainer
from transformers import TrainingArguments

training_args = TrainingArguments(
    per_device_train_batch_size=2,
    gradient_accumulation_steps=4,
    warmup_steps=10,
    num_train_epochs=3,
    learning_rate=2e-4,
    fp16=not torch.cuda.is_bf16_supported(),
    bf16=torch.cuda.is_bf16_supported(),
    logging_steps=25,
    optim="adamw_8bit",
    weight_decay=0.01,
    lr_scheduler_type="linear",
    seed=3407,
    output_dir="medical_qa_outputs",
    save_strategy="epoch",
    save_total_limit=2,
    dataloader_pin_memory=False,
    report_to="none",
)

trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=train_dataset,
    dataset_text_field="text",
    max_seq_length=max_seq_length,
    dataset_num_proc=2,
    args=training_args,
)

print("Trainer configured!")

print("Starting training...")
trainer_stats = trainer.train()
print("Training completed!")

FastLanguageModel.for_inference(model)

test_messages = [
    {"role": "user", "content": "### Question: What is the most common cause of pneumonia in adults?\n### Options: A) Streptococcus pneumoniae B) Haemophilus influenzae C) Mycoplasma pneumoniae D) Legionella pneumophila\n### Answer:"},
]

inputs = tokenizer.apply_chat_template(
    test_messages,
    tokenize=True,
    add_generation_prompt=True,
    return_tensors="pt",
).to("cuda")

outputs = model.generate(
    input_ids=inputs,
    max_new_tokens=256,
    use_cache=True,
    temperature=0.7,
    do_sample=True,
    top_p=0.9,
)

response = tokenizer.batch_decode(outputs)[0]
print("Model Response:")
print(response)

print("Saving model in GGUF format...")
model.save_pretrained_gguf("medical_qa_gguf", tokenizer, quantization_method="q4_k_m")
print("Model saved!")

from google.colab import files
import os

gguf_files = [f for f in os.listdir("medical_qa_gguf") if f.endswith(".gguf")]
if gguf_files:
    gguf_file = os.path.join("medical_qa_gguf", gguf_files[0])
    print(f"Downloading: {gguf_file}")
    files.download(gguf_file)
else:
    print("No GGUF files found!")

print("Fine-tuning process completed!")

